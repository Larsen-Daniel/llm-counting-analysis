{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Qwen 2.5 3B Counting Benchmark + Mediation Analysis\n\nThis notebook runs on Google Colab with T4 GPU:\n1. Benchmarks Qwen 2.5 3B on the counting task\n2. Performs activation patching mediation analysis to identify which layers causally mediate count information"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate torch tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone Repository and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Larsen-Daniel/llm-counting-analysis.git\n",
    "%cd llm-counting-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\nelse:\n    print(\"WARNING: No GPU detected. Model will run on CPU (very slow).\")\n    print(\"Make sure you've enabled GPU in Runtime > Change runtime type\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "def load_dataset(file_path: str) -> List[Dict]:\n",
    "    \"\"\"Load the dataset from JSONL file.\"\"\"\n",
    "    examples = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            examples.append(json.loads(line))\n",
    "    return examples\n",
    "\n",
    "def extract_answer(text: str) -> int | None:\n",
    "    \"\"\"\n",
    "    Extract numerical answer from model output.\n",
    "    Try multiple patterns in order of preference:\n",
    "    1. Last number in parentheses: (N)\n",
    "    2. First number on first line\n",
    "    \"\"\"\n",
    "    # First try to find number in parentheses\n",
    "    matches = re.findall(r'\\((\\d+)\\)', text)\n",
    "    if matches:\n",
    "        return int(matches[-1])  # Return the last match\n",
    "\n",
    "    # Fall back to first number in the output\n",
    "    first_line = text.strip().split('\\n')[0]\n",
    "    number_match = re.search(r'\\d+', first_line)\n",
    "    if number_match:\n",
    "        return int(number_match.group(0))\n",
    "\n",
    "    return None  # No number found\n",
    "\n",
    "def benchmark_qwen(\n",
    "    model_name: str,\n",
    "    dataset: List[Dict],\n",
    "    max_examples: int = None,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Benchmark Qwen model on the counting task.\n",
    "\n",
    "    Args:\n",
    "        model_name: HuggingFace model name\n",
    "        dataset: List of examples\n",
    "        max_examples: Maximum examples to evaluate\n",
    "        device: Device to run on\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Benchmarking: {model_name}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\" if device == \"cuda\" else None,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "    if device == \"cpu\":\n",
    "        model = model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare dataset\n",
    "    if max_examples:\n",
    "        dataset = dataset[:max_examples]\n",
    "\n",
    "    results = {\n",
    "        \"model_id\": model_name,\n",
    "        \"total_examples\": len(dataset),\n",
    "        \"correct\": 0,\n",
    "        \"incorrect\": 0,\n",
    "        \"parse_errors\": 0,\n",
    "        \"numerical_errors\": 0,\n",
    "        \"api_errors\": 0,\n",
    "        \"predictions\": []\n",
    "    }\n",
    "\n",
    "    print(f\"Evaluating on {len(dataset)} examples...\")\n",
    "\n",
    "    for example in tqdm(dataset):\n",
    "        prompt = example[\"prompt\"]\n",
    "        true_answer = example[\"answer\"]\n",
    "\n",
    "        # Generate\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_new_tokens=10,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        generated_text = tokenizer.decode(\n",
    "            outputs[0][inputs.input_ids.shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # Extract answer\n",
    "        predicted_answer = extract_answer(generated_text)\n",
    "\n",
    "        # Check correctness\n",
    "        if predicted_answer is None:\n",
    "            results[\"parse_errors\"] += 1\n",
    "            is_correct = False\n",
    "        else:\n",
    "            is_correct = predicted_answer == true_answer\n",
    "            if is_correct:\n",
    "                results[\"correct\"] += 1\n",
    "            else:\n",
    "                results[\"numerical_errors\"] += 1\n",
    "                results[\"incorrect\"] += 1\n",
    "\n",
    "        # Store prediction\n",
    "        results[\"predictions\"].append({\n",
    "            \"prompt\": prompt,\n",
    "            \"true_answer\": true_answer,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"generated_text\": generated_text,\n",
    "            \"correct\": is_correct\n",
    "        })\n",
    "\n",
    "    # Calculate metrics\n",
    "    results[\"accuracy\"] = results[\"correct\"] / results[\"total_examples\"]\n",
    "    results[\"parse_error_rate\"] = results[\"parse_errors\"] / results[\"total_examples\"]\n",
    "    results[\"numerical_error_rate\"] = results[\"numerical_errors\"] / results[\"total_examples\"]\n",
    "    results[\"api_error_rate\"] = results[\"api_errors\"] / results[\"total_examples\"]\n",
    "\n",
    "    print(f\"\\nResults for {model_name}:\")\n",
    "    print(f\"  Accuracy: {results['accuracy']:.2%} ({results['correct']}/{results['total_examples']})\")\n",
    "    print(f\"  Parse Errors: {results['parse_error_rate']:.2%}\")\n",
    "    print(f\"  Numerical Errors: {results['numerical_error_rate']:.2%}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load dataset\ndataset = load_dataset('data/counting_dataset.jsonl')\nprint(f\"Loaded {len(dataset)} examples\")\n\n# Auto-detect device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# Run benchmark on 1000 examples\nresults = benchmark_qwen(\n    model_name=\"Qwen/Qwen2.5-3B-Instruct\",\n    dataset=dataset,\n    max_examples=1000,\n    device=device\n)\n\n# Save results\nwith open('results/qwen_benchmark_results.json', 'w') as f:\n    json.dump(results, f, indent=2)\n\nprint(\"\\nResults saved to results/qwen_benchmark_results.json\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import files\nfiles.download('results/qwen_benchmark_results.json')"
  },
  {
   "cell_type": "markdown",
   "source": "## Load Model for Mediation Analysis\n\nRun this cell to load the model (if you haven't run the benchmark above, or if you want to reload the model)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load model and tokenizer for mediation analysis\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\nmodel_name = \"Qwen/Qwen2.5-3B-Instruct\"\nprint(f\"Loading {model_name}...\")\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n    device_map=\"auto\" if device == \"cuda\" else None,\n    low_cpu_mem_usage=True\n)\n\nif device == \"cpu\":\n    model = model.to(device)\n\nmodel.eval()\nprint(f\"Model loaded successfully!\")\nprint(f\"Model device: {next(model.parameters()).device}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Pull latest changes and reload\n!git pull origin main\n\n# Import after adding scripts to path\nimport sys\nsys.path.append('scripts')\n\nimport importlib\nimport mediation_utils\nimportlib.reload(mediation_utils)\n\nfrom mediation_utils import run_mediation_analysis\nimport random\n\n# Auto-detect device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# Generate simple synthetic pairs (no need to use dataset)\ndef generate_simple_pairs(n_pairs=500):\n    \"\"\"Generate simple 4-word lists with ~2 matching items (1-3 range).\"\"\"\n    CATEGORIES = {\n        \"fruit\": [\"apple\", \"banana\", \"cherry\", \"grape\", \"orange\"],\n        \"animal\": [\"dog\", \"cat\", \"bird\", \"fish\", \"horse\"],\n        \"tool\": [\"hammer\", \"wrench\", \"saw\", \"drill\", \"pliers\"],\n    }\n    NOISE = [\"bowl\", \"window\", \"door\", \"cloud\", \"mountain\", \"table\", \"chair\", \"lamp\"]\n    \n    pairs = []\n    categories = list(CATEGORIES.keys())\n    \n    for i in range(n_pairs):\n        category = categories[i % len(categories)]\n        cat_words = CATEGORIES[category]\n        \n        # Randomly choose 1-3 matching items (averaging ~2)\n        num_matching = random.randint(1, 3)\n        matching = random.sample(cat_words, num_matching)\n        non_matching = random.sample(NOISE, 4 - num_matching)\n        \n        # Create base list with the matching items\n        base_list = matching + non_matching\n        random.shuffle(base_list)\n        \n        # Low count: first word is noise\n        low_list = base_list.copy()\n        if low_list[0] in cat_words:\n            # Swap first word with a noise word\n            low_list[0] = random.choice([w for w in NOISE if w not in low_list])\n        low_answer = sum(1 for w in low_list if w in cat_words)\n        \n        # High count: first word is matching\n        high_list = base_list.copy()\n        if high_list[0] not in cat_words:\n            # Swap first word with a category word\n            high_list[0] = random.choice([w for w in cat_words if w not in high_list])\n        high_answer = sum(1 for w in high_list if w in cat_words)\n        \n        # Only keep pairs where they differ by exactly 1\n        if high_answer - low_answer != 1:\n            continue\n        \n        prompt_template = \"\"\"Count how many words in the list below match the given type.\n\nType: {category}\nList: {word_list}\n\nIMPORTANT: Respond with ONLY a number in parentheses. Nothing else.\n\nExample:\nType: fruit\nList: apple door banana cloud\nAnswer: (2)\n\nDo NOT write \"2 (2)\" or \"The answer is (2)\" or any other text.\nONLY write: (N)\n\nAnswer: \"\"\"\n        \n        pair_low = {\n            'prompt': prompt_template.format(category=category, word_list=' '.join(low_list)),\n            'word_list': low_list,\n            'category': category,\n            'answer': low_answer\n        }\n        \n        pair_high = {\n            'prompt': prompt_template.format(category=category, word_list=' '.join(high_list)),\n            'word_list': high_list,\n            'category': category,\n            'answer': high_answer\n        }\n        \n        pairs.append((pair_low, pair_high))\n    \n    return pairs\n\n# Generate 500 candidate pairs (will be filtered to 20 perfect ones)\nprint(\"\\nGenerating 500 simple minimal pairs...\")\npairs = generate_simple_pairs(n_pairs=500)\nprint(f\"Generated {len(pairs)} valid pairs\")\n\nprint(f\"\\nRunning mediation analysis (will filter to 20 perfect examples)...\")\nmediation_results = run_mediation_analysis(model, tokenizer, pairs, device=device)\n\n# Save results\nimport json\nwith open('results/mediation_results_qwen3b.json', 'w') as f:\n    json.dump(mediation_results, f, indent=2)\n\n# Display results\nprint(\"\\n\" + \"=\"*80)\nprint(\"TOP 5 LAYERS BY MEAN EFFECT\")\nprint(\"=\"*80)\neffects = [(int(i), data['mean_effect']) for i, data in mediation_results['layer_effects'].items()]\neffects.sort(key=lambda x: x[1], reverse=True)\nfor layer, effect in effects[:5]:\n    print(f\"  Layer {layer}: {effect:.3f}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"SUMMARY\")\nprint(\"=\"*80)\nprint(f\"Baseline accuracy: {mediation_results['baseline_accuracy']:.1%}\")\nprint(f\"Number of pairs tested: {mediation_results['n_pairs']}\")\n\nfrom google.colab import files\nfiles.download('results/mediation_results_qwen3b.json')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}