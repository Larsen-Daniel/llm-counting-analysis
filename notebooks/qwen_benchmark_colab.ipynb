{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Qwen 2.5 3B Counting Benchmark + Mediation Analysis\n\nThis notebook runs on Google Colab with T4 GPU:\n1. Benchmarks Qwen 2.5 3B on the counting task\n2. Performs activation patching mediation analysis to identify which layers causally mediate count information"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate torch tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone Repository and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Larsen-Daniel/llm-counting-analysis.git\n",
    "%cd llm-counting-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\nelse:\n    print(\"WARNING: No GPU detected. Model will run on CPU (very slow).\")\n    print(\"Make sure you've enabled GPU in Runtime > Change runtime type\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "def load_dataset(file_path: str) -> List[Dict]:\n",
    "    \"\"\"Load the dataset from JSONL file.\"\"\"\n",
    "    examples = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            examples.append(json.loads(line))\n",
    "    return examples\n",
    "\n",
    "def extract_answer(text: str) -> int | None:\n",
    "    \"\"\"\n",
    "    Extract numerical answer from model output.\n",
    "    Try multiple patterns in order of preference:\n",
    "    1. Last number in parentheses: (N)\n",
    "    2. First number on first line\n",
    "    \"\"\"\n",
    "    # First try to find number in parentheses\n",
    "    matches = re.findall(r'\\((\\d+)\\)', text)\n",
    "    if matches:\n",
    "        return int(matches[-1])  # Return the last match\n",
    "\n",
    "    # Fall back to first number in the output\n",
    "    first_line = text.strip().split('\\n')[0]\n",
    "    number_match = re.search(r'\\d+', first_line)\n",
    "    if number_match:\n",
    "        return int(number_match.group(0))\n",
    "\n",
    "    return None  # No number found\n",
    "\n",
    "def benchmark_qwen(\n",
    "    model_name: str,\n",
    "    dataset: List[Dict],\n",
    "    max_examples: int = None,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Benchmark Qwen model on the counting task.\n",
    "\n",
    "    Args:\n",
    "        model_name: HuggingFace model name\n",
    "        dataset: List of examples\n",
    "        max_examples: Maximum examples to evaluate\n",
    "        device: Device to run on\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Benchmarking: {model_name}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\" if device == \"cuda\" else None,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "    if device == \"cpu\":\n",
    "        model = model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare dataset\n",
    "    if max_examples:\n",
    "        dataset = dataset[:max_examples]\n",
    "\n",
    "    results = {\n",
    "        \"model_id\": model_name,\n",
    "        \"total_examples\": len(dataset),\n",
    "        \"correct\": 0,\n",
    "        \"incorrect\": 0,\n",
    "        \"parse_errors\": 0,\n",
    "        \"numerical_errors\": 0,\n",
    "        \"api_errors\": 0,\n",
    "        \"predictions\": []\n",
    "    }\n",
    "\n",
    "    print(f\"Evaluating on {len(dataset)} examples...\")\n",
    "\n",
    "    for example in tqdm(dataset):\n",
    "        prompt = example[\"prompt\"]\n",
    "        true_answer = example[\"answer\"]\n",
    "\n",
    "        # Generate\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_new_tokens=10,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        generated_text = tokenizer.decode(\n",
    "            outputs[0][inputs.input_ids.shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # Extract answer\n",
    "        predicted_answer = extract_answer(generated_text)\n",
    "\n",
    "        # Check correctness\n",
    "        if predicted_answer is None:\n",
    "            results[\"parse_errors\"] += 1\n",
    "            is_correct = False\n",
    "        else:\n",
    "            is_correct = predicted_answer == true_answer\n",
    "            if is_correct:\n",
    "                results[\"correct\"] += 1\n",
    "            else:\n",
    "                results[\"numerical_errors\"] += 1\n",
    "                results[\"incorrect\"] += 1\n",
    "\n",
    "        # Store prediction\n",
    "        results[\"predictions\"].append({\n",
    "            \"prompt\": prompt,\n",
    "            \"true_answer\": true_answer,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"generated_text\": generated_text,\n",
    "            \"correct\": is_correct\n",
    "        })\n",
    "\n",
    "    # Calculate metrics\n",
    "    results[\"accuracy\"] = results[\"correct\"] / results[\"total_examples\"]\n",
    "    results[\"parse_error_rate\"] = results[\"parse_errors\"] / results[\"total_examples\"]\n",
    "    results[\"numerical_error_rate\"] = results[\"numerical_errors\"] / results[\"total_examples\"]\n",
    "    results[\"api_error_rate\"] = results[\"api_errors\"] / results[\"total_examples\"]\n",
    "\n",
    "    print(f\"\\nResults for {model_name}:\")\n",
    "    print(f\"  Accuracy: {results['accuracy']:.2%} ({results['correct']}/{results['total_examples']})\")\n",
    "    print(f\"  Parse Errors: {results['parse_error_rate']:.2%}\")\n",
    "    print(f\"  Numerical Errors: {results['numerical_error_rate']:.2%}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load dataset\ndataset = load_dataset('data/counting_dataset.jsonl')\nprint(f\"Loaded {len(dataset)} examples\")\n\n# Auto-detect device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# Run benchmark on 1000 examples\nresults = benchmark_qwen(\n    model_name=\"Qwen/Qwen2.5-3B-Instruct\",\n    dataset=dataset,\n    max_examples=1000,\n    device=device\n)\n\n# Save results\nwith open('results/qwen_benchmark_results.json', 'w') as f:\n    json.dump(results, f, indent=2)\n\nprint(\"\\nResults saved to results/qwen_benchmark_results.json\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import files\nfiles.download('results/qwen_benchmark_results.json')"
  },
  {
   "cell_type": "markdown",
   "source": "## Load Model for Mediation Analysis\n\nRun this cell to load the model (if you haven't run the benchmark above, or if you want to reload the model)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load model and tokenizer for mediation analysis\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\nmodel_name = \"Qwen/Qwen2.5-3B-Instruct\"\nprint(f\"Loading {model_name}...\")\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n    device_map=\"auto\" if device == \"cuda\" else None,\n    low_cpu_mem_usage=True\n)\n\nif device == \"cpu\":\n    model = model.to(device)\n\nmodel.eval()\nprint(f\"Model loaded successfully!\")\nprint(f\"Model device: {next(model.parameters()).device}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Pull latest changes and reload\n!git pull origin main\n\n# Import after adding scripts to path\nimport sys\nsys.path.append('scripts')\n\nimport importlib\nimport mediation_utils\nimportlib.reload(mediation_utils)\n\nfrom mediation_utils import run_mediation_analysis, extract_answer\nimport json\nimport random\n\n# Auto-detect device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# Load dataset\nprint(\"\\nLoading dataset...\")\ndataset = []\nwith open('data/counting_dataset.jsonl', 'r') as f:\n    for line in f:\n        dataset.append(json.loads(line))\nprint(f\"Loaded {len(dataset)} examples\")\n\n# Generate simple synthetic pairs - SAME AS BENCHMARK TEST\ndef generate_simple_pairs(n_pairs=500):\n    \"\"\"Generate pairs matching dataset format: 8-10 word lists with 2-3 matching items.\"\"\"\n    CATEGORIES = {\n        \"fruit\": [\"apple\", \"banana\", \"cherry\", \"grape\", \"orange\", \"mango\", \"kiwi\", \"peach\", \"pear\", \"plum\",\n                  \"strawberry\", \"blueberry\", \"watermelon\", \"pineapple\", \"lemon\"],\n        \"animal\": [\"dog\", \"cat\", \"bird\", \"fish\", \"horse\", \"cow\", \"pig\", \"sheep\", \"goat\", \"chicken\",\n                   \"rabbit\", \"mouse\", \"elephant\", \"lion\", \"tiger\"],\n        \"vehicle\": [\"car\", \"bus\", \"truck\", \"bike\", \"train\", \"plane\", \"boat\", \"ship\", \"motorcycle\", \"scooter\",\n                    \"van\", \"taxi\", \"subway\", \"helicopter\", \"tram\"],\n        \"color\": [\"red\", \"blue\", \"green\", \"yellow\", \"orange\", \"purple\", \"pink\", \"brown\", \"black\", \"white\",\n                  \"gray\", \"violet\", \"indigo\", \"turquoise\", \"crimson\"],\n        \"tool\": [\"hammer\", \"screwdriver\", \"wrench\", \"pliers\", \"saw\", \"drill\", \"chisel\", \"axe\", \"knife\", \"scissors\",\n                 \"ruler\", \"tape\", \"level\", \"clamp\", \"file\"],\n        \"furniture\": [\"chair\", \"table\", \"sofa\", \"bed\", \"desk\", \"cabinet\", \"shelf\", \"dresser\", \"bench\", \"stool\",\n                      \"wardrobe\", \"bookcase\", \"ottoman\", \"nightstand\", \"couch\"],\n        \"clothing\": [\"shirt\", \"pants\", \"dress\", \"skirt\", \"jacket\", \"coat\", \"sweater\", \"hat\", \"shoes\", \"socks\",\n                     \"scarf\", \"gloves\", \"belt\", \"tie\", \"jeans\"],\n        \"food\": [\"pizza\", \"burger\", \"pasta\", \"rice\", \"bread\", \"cheese\", \"salad\", \"soup\", \"sandwich\", \"taco\",\n                 \"noodles\", \"steak\", \"chicken\", \"fish\", \"potato\"],\n        \"sport\": [\"soccer\", \"basketball\", \"tennis\", \"baseball\", \"football\", \"hockey\", \"volleyball\", \"golf\", \"cricket\", \"rugby\",\n                  \"swimming\", \"boxing\", \"skiing\", \"skating\", \"running\"],\n        \"instrument\": [\"guitar\", \"piano\", \"drums\", \"violin\", \"flute\", \"trumpet\", \"saxophone\", \"cello\", \"clarinet\", \"harp\",\n                       \"trombone\", \"banjo\", \"accordion\", \"oboe\", \"tuba\"]\n    }\n\n    NOISE_WORDS = [\"bowl\", \"window\", \"door\", \"cloud\", \"mountain\", \"river\", \"tree\", \"rock\", \"sand\", \"metal\",\n                   \"plastic\", \"glass\", \"paper\", \"wood\", \"stone\", \"gold\", \"silver\", \"copper\", \"iron\", \"steel\",\n                   \"number\", \"letter\", \"word\", \"sentence\", \"paragraph\", \"page\", \"book\", \"magazine\", \"newspaper\", \"document\"]\n\n    pairs = []\n    categories = list(CATEGORIES.keys())\n\n    for i in range(n_pairs):\n        category = categories[i % len(categories)]\n        cat_words = CATEGORIES[category]\n\n        list_length = random.randint(8, 10)\n        num_matching = random.randint(2, 3)\n        matching = random.sample(cat_words, num_matching)\n\n        non_matching_pool = NOISE_WORDS.copy()\n        for cat, items in CATEGORIES.items():\n            if cat != category:\n                non_matching_pool.extend(items)\n\n        non_matching = random.sample(non_matching_pool, list_length - num_matching)\n\n        base_list = matching + non_matching\n        random.shuffle(base_list)\n\n        low_list = base_list.copy()\n        if low_list[0] in cat_words:\n            available_noise = [w for w in non_matching_pool if w not in low_list and w not in cat_words]\n            if not available_noise:\n                continue\n            low_list[0] = random.choice(available_noise)\n        low_answer = sum(1 for w in low_list if w in cat_words)\n\n        high_list = base_list.copy()\n        if high_list[0] not in cat_words:\n            available_cat = [w for w in cat_words if w not in high_list]\n            if not available_cat:\n                continue\n            high_list[0] = random.choice(available_cat)\n        high_answer = sum(1 for w in high_list if w in cat_words)\n\n        if high_answer - low_answer != 1:\n            continue\n\n        prompt_template = \"\"\"Count how many words in the list below match the given type.\n\nType: {category}\nList: {word_list}\n\nYOU MUST respond with ONLY a number in parentheses, like this: (5)\nDo NOT include any other text, explanations, or words.\nJust output the number in parentheses and nothing else.\n\nAnswer: \"\"\"\n\n        pair_low = {\n            'prompt': prompt_template.format(category=category, word_list=' '.join(low_list)),\n            'word_list': low_list,\n            'category': category,\n            'answer': low_answer\n        }\n\n        pair_high = {\n            'prompt': prompt_template.format(category=category, word_list=' '.join(high_list)),\n            'word_list': high_list,\n            'category': category,\n            'answer': high_answer\n        }\n\n        pairs.append((pair_low, pair_high))\n\n    return pairs\n\n# Generate 2000 candidate pairs (will be filtered to 100 perfect ones)\nprint(\"\\nGenerating synthetic minimal pairs...\")\npairs = generate_simple_pairs(n_pairs=2000)\nprint(f\"Generated {len(pairs)} valid pairs\")\n\nprint(f\"\\nRunning mediation analysis (will filter to 100 perfect examples)...\")\nmediation_results = run_mediation_analysis(model, tokenizer, pairs, device=device)\n\n# Save results\nwith open('results/mediation_results_qwen3b.json', 'w') as f:\n    json.dump(mediation_results, f, indent=2)\n\n# Display results\nprint(\"\\n\" + \"=\"*80)\nprint(\"TOP 5 LAYERS BY MEAN EFFECT\")\nprint(\"=\"*80)\neffects = [(int(i), data['mean_effect']) for i, data in mediation_results['layer_effects'].items()]\neffects.sort(key=lambda x: x[1], reverse=True)\nfor layer, effect in effects[:5]:\n    print(f\"  Layer {layer}: {effect:.3f}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"SUMMARY\")\nprint(\"=\"*80)\nprint(f\"Baseline accuracy: {mediation_results['baseline_accuracy']:.1%}\")\nprint(f\"Number of pairs tested: {mediation_results['n_pairs']}\")\n\nfrom google.colab import files\nfiles.download('results/mediation_results_qwen3b.json')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}