{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Qwen 2.5 3B Counting Benchmark on Google Colab\n\nThis notebook benchmarks Qwen/Qwen2.5-3B-Instruct on the counting task (fits comfortably on T4 GPU)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate torch tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone Repository and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Larsen-Daniel/llm-counting-analysis.git\n",
    "%cd llm-counting-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "def load_dataset(file_path: str) -> List[Dict]:\n",
    "    \"\"\"Load the dataset from JSONL file.\"\"\"\n",
    "    examples = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            examples.append(json.loads(line))\n",
    "    return examples\n",
    "\n",
    "def extract_answer(text: str) -> int | None:\n",
    "    \"\"\"\n",
    "    Extract numerical answer from model output.\n",
    "    Try multiple patterns in order of preference:\n",
    "    1. Last number in parentheses: (N)\n",
    "    2. First number on first line\n",
    "    \"\"\"\n",
    "    # First try to find number in parentheses\n",
    "    matches = re.findall(r'\\((\\d+)\\)', text)\n",
    "    if matches:\n",
    "        return int(matches[-1])  # Return the last match\n",
    "\n",
    "    # Fall back to first number in the output\n",
    "    first_line = text.strip().split('\\n')[0]\n",
    "    number_match = re.search(r'\\d+', first_line)\n",
    "    if number_match:\n",
    "        return int(number_match.group(0))\n",
    "\n",
    "    return None  # No number found\n",
    "\n",
    "def benchmark_qwen(\n",
    "    model_name: str,\n",
    "    dataset: List[Dict],\n",
    "    max_examples: int = None,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Benchmark Qwen model on the counting task.\n",
    "\n",
    "    Args:\n",
    "        model_name: HuggingFace model name\n",
    "        dataset: List of examples\n",
    "        max_examples: Maximum examples to evaluate\n",
    "        device: Device to run on\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Benchmarking: {model_name}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\" if device == \"cuda\" else None,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "    if device == \"cpu\":\n",
    "        model = model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare dataset\n",
    "    if max_examples:\n",
    "        dataset = dataset[:max_examples]\n",
    "\n",
    "    results = {\n",
    "        \"model_id\": model_name,\n",
    "        \"total_examples\": len(dataset),\n",
    "        \"correct\": 0,\n",
    "        \"incorrect\": 0,\n",
    "        \"parse_errors\": 0,\n",
    "        \"numerical_errors\": 0,\n",
    "        \"api_errors\": 0,\n",
    "        \"predictions\": []\n",
    "    }\n",
    "\n",
    "    print(f\"Evaluating on {len(dataset)} examples...\")\n",
    "\n",
    "    for example in tqdm(dataset):\n",
    "        prompt = example[\"prompt\"]\n",
    "        true_answer = example[\"answer\"]\n",
    "\n",
    "        # Generate\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_new_tokens=10,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        generated_text = tokenizer.decode(\n",
    "            outputs[0][inputs.input_ids.shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # Extract answer\n",
    "        predicted_answer = extract_answer(generated_text)\n",
    "\n",
    "        # Check correctness\n",
    "        if predicted_answer is None:\n",
    "            results[\"parse_errors\"] += 1\n",
    "            is_correct = False\n",
    "        else:\n",
    "            is_correct = predicted_answer == true_answer\n",
    "            if is_correct:\n",
    "                results[\"correct\"] += 1\n",
    "            else:\n",
    "                results[\"numerical_errors\"] += 1\n",
    "                results[\"incorrect\"] += 1\n",
    "\n",
    "        # Store prediction\n",
    "        results[\"predictions\"].append({\n",
    "            \"prompt\": prompt,\n",
    "            \"true_answer\": true_answer,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"generated_text\": generated_text,\n",
    "            \"correct\": is_correct\n",
    "        })\n",
    "\n",
    "    # Calculate metrics\n",
    "    results[\"accuracy\"] = results[\"correct\"] / results[\"total_examples\"]\n",
    "    results[\"parse_error_rate\"] = results[\"parse_errors\"] / results[\"total_examples\"]\n",
    "    results[\"numerical_error_rate\"] = results[\"numerical_errors\"] / results[\"total_examples\"]\n",
    "    results[\"api_error_rate\"] = results[\"api_errors\"] / results[\"total_examples\"]\n",
    "\n",
    "    print(f\"\\nResults for {model_name}:\")\n",
    "    print(f\"  Accuracy: {results['accuracy']:.2%} ({results['correct']}/{results['total_examples']})\")\n",
    "    print(f\"  Parse Errors: {results['parse_error_rate']:.2%}\")\n",
    "    print(f\"  Numerical Errors: {results['numerical_error_rate']:.2%}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load dataset\ndataset = load_dataset('data/counting_dataset.jsonl')\nprint(f\"Loaded {len(dataset)} examples\")\n\n# Run benchmark on 1000 examples\nresults = benchmark_qwen(\n    model_name=\"Qwen/Qwen2.5-3B-Instruct\",\n    dataset=dataset,\n    max_examples=1000\n)\n\n# Save results\nwith open('results/qwen_benchmark_results.json', 'w') as f:\n    json.dump(results, f, indent=2)\n\nprint(\"\\nResults saved to results/qwen_benchmark_results.json\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import files\nfiles.download('results/qwen_benchmark_results.json')"
  },
  {
   "cell_type": "markdown",
   "source": "## Mediation Analysis (Optional)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Pull latest changes and reload\n!git pull origin main\n\n# Import after adding scripts to path\nimport sys\nsys.path.append('scripts')\n\nimport importlib\nimport mediation_utils\nimportlib.reload(mediation_utils)\n\n# Mediation Analysis\nfrom mediation_utils import generate_minimal_pairs, run_mediation_analysis\n\nprint(\"\\nGenerating minimal pairs...\")\npairs = generate_minimal_pairs(dataset, n_pairs=200)\n\nprint(f\"Running mediation analysis on {len(pairs)} pairs...\")\nmediation_results = run_mediation_analysis(model, tokenizer, pairs, device='cuda')\n\nwith open('results/mediation_results_qwen3b.json', 'w') as f:\n    json.dump(mediation_results, f, indent=2)\n\nprint(\"\\nTop 5 layers by causal effect:\")\neffects = [(int(i), data['mean_effect']) for i, data in mediation_results['layer_effects'].items()]\neffects.sort(key=lambda x: x[1], reverse=True)\nfor layer, effect in effects[:5]:\n    print(f\"  Layer {layer}: {effect:.3f}\")\n\nfiles.download('results/mediation_results_qwen3b.json')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}